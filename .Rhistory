F = ((SSW - SSpe) / (ga$df.residual - g$df.residual)) / (SSpe/ga$df.residual)
F
SSW = sum(g$residuals^2)
SSpe = sum(ga$residuals^2)
F = ((SSW - SSpe) / (ga$df.residual - g$df.residual)) / (SSpe/ga$df.residual)
F
((SSW - SSpe) / (ga$df.residual - g$df.residual))
F = ((SSW - SSpe) / (g$df.residual - ga$df.residual)) / (SSpe/ga$df.residual)
F
1 - pf(D, (g$df.residual - ga$df.residual), ga$df.residual)
1 - pf(F, (g$df.residual - ga$df.residual), ga$df.residual)
# Easier way of doing the lack of fit test
anova(g, ga)
savings <- read.table("savings.data") # read the data into R
g <- lm(sav ~ p15 + p75 + inc + gro, data=savings) # fit the model with sav as the response and the rest variables as predictors
plot(g$res, ylab="Residual", main="index plot of residuals") # draw the plot of residuals against index
savings <- read.table("savings.data") # read the data into R
g <- lm(sav ~ p15 + p75 + inc + gro, data=savings) # fit the model with sav as the response and the rest variables as predictors
plot(g$res, ylab="Residual", main="index plot of residuals") # draw the plot of residuals against index
# Let's find which countries correspond to the largest and smallest residuals.
sort(g$res)[c(1,50)] # sort the residuals from small to large and get the largest and smallest residuals; the command "sort" returns a vector in ascending or descending order
?identify
# We first extract the X-matrix here using "model.matrix()" and then compute and plot the leverages (also called "hat" values)
x = model.matrix(g)
lev = hat(x)
lev
x
lev <- lm.influence(g)$hat
lev
lev = hat(x)
plot(lev, ylab="Leverages",main="index plot of leverages") # draw the plot of leverages against index
abline(h=2*5/50) # add a 2p/n horizontal line
names(lev) = countries
savings <- read.table("savings.data") # read the data into R
g <- lm(sav ~ p15 + p75 + inc + gro, data=savings) # fit the model with sav as the response and the rest variables as predictors
plot(g$res, ylab="Residual", main="index plot of residuals") # draw the plot of residuals against index
# Let's find which countries correspond to the largest and smallest residuals.
sort(g$res)[c(1,50)] # sort the residuals from small to large and get the largest and smallest residuals; the command "sort" returns a vector in ascending or descending order
countries <- row.names(savings) # extract the country names
identify(1:50,g$res,countries) # label the points with their country names
# We first extract the X-matrix here using "model.matrix()" and then compute and plot the leverages (also called "hat" values)
x = model.matrix(g)
lev = hat(x)
plot(lev, ylab="Leverages",main="index plot of leverages") # draw the plot of leverages against index
abline(h=2*5/50) # add a 2p/n horizontal line
names(lev) = countries
savings
View(savings)
savings <- read.table("savings.data") # read the data into R
g <- lm(sav ~ p15 + p75 + inc + gro, data=savings) # fit the model with sav as the response and the rest variables as predictors
plot(g$res, ylab="Residual", main="index plot of residuals") # draw the plot of residuals against index
# Let's find which countries correspond to the largest and smallest residuals.
sort(g$res)[c(1,50)] # sort the residuals from small to large and get the largest and smallest residuals; the command "sort" returns a vector in ascending or descending order
countries <- row.names(savings) # extract the country names
identify(1:50,g$res,countries) # label the points with their country names
# We first extract the X-matrix here using "model.matrix()" and then compute and plot the leverages (also called "hat" values)
x = model.matrix(g)
lev = hat(x)
plot(lev, ylab="Leverages",main="index plot of leverages") # draw the plot of leverages against index
abline(h=2*5/50) # add a 2p/n horizontal line
names(lev) = countries
identify(1:50, lev, countries)
plot(lev, ylab="Leverages",main="index plot of leverages") # draw the plot of leverages against index
abline(h=2*5/50) # add a 2p/n horizontal line
names(lev) = countries
identify(1:50, lev, countries)
lev[lev> 2 * sum(lev)/50]
# Studentized residuals
gsum <- summary(g) # get the summary output of the fitted model g
gsum$sig # The σ-hat
# Compute studentized residuals
stud = g$residuals / (gsum$sigma * sqrt(1 - lev))
plot(stud,ylab="studentized Residuals",main="studentized residuals")
# Easy way to compoute studentized residuals
rstandard(g)
# Easy way to compoute studentized residuals
var(rstandard(g))
# Outlier and jacknife residuals
jack = rstudent(g)
jack
# Here is an example of a dataset with multiple outliers.
star <- read.table("star.data",h=T) # read the data into R
plot(star$temp,star$light) # draw the scatter plot of log light intensity against temperature
# Influential observation
cook = cooks.distance(g)
?identify
cook = cooks.distance(g)
plot(cook, ylab="Cooks distance") # draw the plot of Cook's statistics against index
identify(1:50,cook,countries) # identify the three countries with largest Cook's statistics\
source("~/R/Linear Model/Diagnostics 1.R", echo=TRUE)
install.packages("RSQLite")
install.packages("globals")
install.packages("esquisse")
esquisse:::esquisser()
# Location and/or Scale Changes
savings <- read.table("savings.data")
g <- lm(sav ~ p15 + p75 + inc + gro, data=savings)
summary(g)
# The coefficient for income is rather small --- let's measure income in thousands of dollars instead and refit:
g = lm(sav ~ p15 + p75 + (inc/1000) + gro, data=savings)
# The coefficient for income is rather small --- let's measure income in thousands of dollars instead and refit:
g = lm(sav ~ p15 + p75 + I(inc/1000) + gro, data=savings)
g
i
# One rather thorough approach to scaling is to convert all the variables to standard unit (mean=0 and variance=1) using the "scale" command:
scale(savings)
str(savings)
# One rather thorough approach to scaling is to convert all the variables to standard unit (mean=0 and variance=1) using the "scale" command:
scav = scale(savings)
mean(scav)
# One rather thorough approach to scaling is to convert all the variables to standard unit (mean=0 and variance=1) using the "scale" command:
scav = scale(data.frame(savings))
mean(scav)
scav
str(scav)
# One rather thorough approach to scaling is to convert all the variables to standard unit (mean=0 and variance=1) using the "scale" command:
scav = data.frame(scale(savings))
mean(scav)
# One rather thorough approach to scaling is to convert all the variables to standard unit (mean=0 and variance=1) using the "scale" command:
scsav = data.frame(scale(savings))
mean(scsav)
mean(scsav)
scsav
cov(scsav)
# Polynomial Regression
savings <- read.table("savings.data")
plot(savings$gro, savings$sav)
par(mfrow = c(2,2))
plot(savings$gro, savings$sav)
par(mfrow = c(1,1))
plot(savings$gro, savings$sav)
#First fit a linear model:
summary(lm(sav ~ gro, data=savings))
# P-value of gro is significant so move on to a quadratic term:
summary(lm(sav ~ gro + I(gro^2), data=savings))
# Again p-value of gro^2 is significant so move on to a cubic term:
summary(lm(sav ~ gro + I(gro^2) + I(gro^3), data=savings))
# You have to refit the model each time a term is removed. Orthogonal polynomials save some time in this process - the "poly()" function in R constructs these:
g = lm(sav ~ poly(gro, 4), data = savings)
summary(g)
summary(g, correlation = TRUE)
model.matrix(lm(sav ~ poly(gro,inc, degree=2), data=savings))
summary(lm(sav ~ poly(gro,inc, degree=2), data=savings))
?poly
?segments
predict(object = g1, c(30))
# Broken Stick Regression
# In the analysis of the savings data, we observed that there were two groups in the data
#
# We might want to fit a different model to the two parts.
g1 <- lm(sav~p15, data=savings, subset=(p15<35))
predict(object = g1, c(30))
# Broken Stick Regression
# In the analysis of the savings data, we observed that there were two groups in the data
#
# We might want to fit a different model to the two parts.
g1 <- lm(sav~p15, data=savings, subset=(p15<35))
g2 = lm(sav~p15, data=savings, subset=(p15<35))
plot(savings$p15, savings$sav, xlab = "Pop'n under 15", ylab="savings rate")
abline(v=35, lty=5)
segments(20, g1$coef[1]+g1$coef[2]*20, 35, g1$coef[1]+g1$coef[2]*35)
segments(48, g2$coef[1]+g2$coef[2]*48, 35, g2$coef[1]+g2$coef[2]*35)
summary(g1)
g2 = lm(sav~p15, data=savings, subset=(p15>35))
segments(48, g2$coef[1]+g2$coef[2]*48, 35, g2$coef[1]+g2$coef[2]*35)
segments(20, g1$coef[1]+g1$coef[2]*20, 35, g1$coef[1]+g1$coef[2]*35)
# Broken Stick Regression
# In the analysis of the savings data, we observed that there were two groups in the data
#
# We might want to fit a different model to the two parts.
g1 <- lm(sav~p15, data=savings, subset=(p15<35))
g2 = lm(sav~p15, data=savings, subset=(p15>35))
plot(savings$p15, savings$sav, xlab = "Pop'n under 15", ylab="savings rate")
abline(v=35, lty=5)
segments(20, g1$coef[1]+g1$coef[2]*20, 35, g1$coef[1]+g1$coef[2]*35)
segments(48, g2$coef[1]+g2$coef[2]*48, 35, g2$coef[1]+g2$coef[2]*35)
g1
g2
?lm
model.matrix(g1)
savings$p15[p15 > 35]
savings$p15>35
savings$p15[savings$p15>35]
model.matrix(g2)
d = function(x) {
ifelse(x<35, 0, 1)
}
d
gb <- lm(sav ~ p15+I((p15-35)*d(p15)), data=savings)
x <- seq(20, 48, by=1)
py <- gb$coef[1]+gb$coef[2]*x+gb$coef[3]*((x-35)*d(x))
lines(x, py, lty=2)
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
y = sin(2 * pi * x^3)^3 + rnorm(0, 0.01)
return(y)
}
funky(100)
funky(1)
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
sin(2 * pi * x^3)^3 + rnorm(0, 0.01)
}
funky(10)
funky(3)
funky(3)
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) sin(2 * pi * x^3)^3 + rnorm(0, 0.01)
funky(3)
funky(3)
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) sin(2 * pi * x^3)^3
funky(3)
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
sin(2 * pi * x^3)^3
}
funky(3)
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
sin(2 * pi * x^3)^3 + rnorm(1, mean = 0, sd = 0.01)
}
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
y = sin(2 * pi * x^3)^3 + rnorm(1, mean = 0, sd = 0.01)
return(y)
}
funky(2)
x = seq(0, 1 by = 0.01)
x = seq(0, 1, by = 0.01)
y = funky(x)
y
matplot(x,cbind(y,funky(x)),type="pl",ylab="y",pch=18,lty=1,main="True Model")
y = funky(x) + + rnorm(101, mean = 0, sd = 0.01)
matplot(x,cbind(y,funky(x)),type="pl",ylab="y",pch=18,lty=1,main="True Model")
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
y = sin(2 * pi * x^3)^3
return(y)
}
x = seq(0, 1, by = 0.01)
y = funky(x) + + rnorm(101, mean = 0, sd = 0.01)
matplot(x,cbind(y,funky(x)),type="pl",ylab="y",pch=18,lty=1,main="True Model")
# Regression Spline
# Let's see how the regression spline method perform on a constructed example.
#
# Suppose we know the true model is:
#
# y=sin3(2πx3)+ε,  ε~N(0, 0.01).
#
# The advantage of using simulated data is that we can see how close our methods come to the truth. We generate the data and display it in plots:
funky = function(x) {
y = sin(2 * pi * x^3)^3
return(y)
}
x = seq(0, 1, by = 0.01)
y = funky(x)  + rnorm(101, mean = 0, sd = 0.01)
matplot(x,cbind(y,funky(x)),type="pl",ylab="y",pch=18,lty=1,main="True Model")
# We see how an orthogonal polynomial bases of orders 4, 9, and 12 do in fitting this data:
g4 <- lm(y~poly(x,4))
g9 <- lm(y~poly(x,9))
g12 <- lm(y~poly(x,12))
matplot(x,cbind(y,g4$fit,g9$fit,g12$fit),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="orthogonal polynomials")
# We see that order 4 is a clear lack-of-fit.
#
# Order 12 is much better although the fit is too wiggly in the first section and misses the point of inflection.
#We now create the B-spline basis. You need to have three additional knots at the start and end to get the right basis. I have chosen to the knot locations to put more in regions of greater curvature. I have used 12 basis functions for comparability to the orthogonal polynomial fit.
library(splines)
# We see that order 4 is a clear lack-of-fit.
#
# Order 12 is much better although the fit is too wiggly in the first section and misses the point of inflection.
#We now create the B-spline basis. You need to have three additional knots at the start and end to get the right basis. I have chosen to the knot locations to put more in regions of greater curvature. I have used 12 basis functions for comparability to the orthogonal polynomial fit.
library(splines)
knots = c(0,0,0,0,0.2,0.4,0.5,0.6,0.7,0.8,0.85,0.9,1,1,1,1)
bx = splineDesign(knots, x)
bx
gs = lm(y ~ bx)
matplot(x,cbind(y,funky(x),gs$fit),type="pll",ylab="y",pch=18,lty=1,main="Spline fit")
funky = function(x) {
y = sin(2 * pi * x^3)^3
return(y)
}
x = seq(0, 1, by = 0.01)
y = funky(x)  + 0.1 * rnorm(101)
matplot(x,cbind(y,funky(x)),type="pl",ylab="y",pch=18,lty=1,main="True Model")
# We see how an orthogonal polynomial bases of orders 4, 9, and 12 do in fitting this data:
g4 <- lm(y~poly(x,4))
g9 <- lm(y~poly(x,9))
g12 <- lm(y~poly(x,12))
matplot(x,cbind(y,g4$fit,g9$fit,g12$fit),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="orthogonal polynomials")
# We see that order 4 is a clear lack-of-fit.
#
# Order 12 is much better although the fit is too wiggly in the first section and misses the point of inflection.
#We now create the B-spline basis. You need to have three additional knots at the start and end to get the right basis. I have chosen to the knot locations to put more in regions of greater curvature. I have used 12 basis functions for comparability to the orthogonal polynomial fit.
library(splines)
knots = c(0,0,0,0,0.2,0.4,0.5,0.6,0.7,0.8,0.85,0.9,1,1,1,1)
bx = splineDesign(knots, x)
gs = lm(y ~ bx)
matplot(x,cbind(y,funky(x),gs$fit),type="pll",ylab="y",pch=18,lty=1,main="Spline fit")
funky = function(x) {
y = sin(2 * pi * x^3)^3
return(y)
}
x = seq(0, 1, by = 0.01)
y = funky(x)  + rnorm(101 ,mean = 0, sd = 0.1)
matplot(x,cbind(y,funky(x)),type="pl",ylab="y",pch=18,lty=1,main="True Model")
# We see how an orthogonal polynomial bases of orders 4, 9, and 12 do in fitting this data:
g4 <- lm(y~poly(x,4))
g9 <- lm(y~poly(x,9))
g12 <- lm(y~poly(x,12))
matplot(x,cbind(y,g4$fit,g9$fit,g12$fit),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="orthogonal polynomials")
# We see that order 4 is a clear lack-of-fit.
#
# Order 12 is much better although the fit is too wiggly in the first section and misses the point of inflection.
#We now create the B-spline basis. You need to have three additional knots at the start and end to get the right basis. I have chosen to the knot locations to put more in regions of greater curvature. I have used 12 basis functions for comparability to the orthogonal polynomial fit.
library(splines)
knots = c(0,0,0,0,0.2,0.4,0.5,0.6,0.7,0.8,0.85,0.9,1,1,1,1)
bx = splineDesign(knots, x)
gs = lm(y ~ bx)
matplot(x,cbind(y,funky(x),gs$fit),type="pll",ylab="y",pch=18,lty=1,main="Spline fit")
ls1 <- lowess(x, y, f=0.05)
ls2 <- lowess(x, y, f=0.2)
ls3 <- lowess(x, y, f=0.4)
ls4 <- lowess(x, y, f=0.6)
1s1
ls1
?lowess
> matplot(x,cbind(y,ls1$y,ls2$y,ls3$y),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="LOWESS fit")
matplot(x,cbind(y,ls1$y,ls2$y,ls3$y),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="LOWESS fit")
matplot(x,cbind(y,ls1$y),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="LOWESS fit")
matplot(x,cbind(y,,ls1$y,ls2$y,ls3$y,ls4$y)),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="LOWESS fit")
matplot(x,cbind(y,ls1$y,ls2$y,ls3$y,ls4$y)),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="LOWESS fit")
matplot(x,cbind(y,ls1$y,ls2$y,ls3$y,ls4$y),type="plll",ylab="y",pch=18,lty=c(1,2,3),main="LOWESS fit")
# The LOESS smoother in R:
loe1 <- loess(y~x, span=0.1)
loe2 <- loess(y~x, span=0.3)
loe3 <- loess(y~x, span=0.5)
loe4 <- loess(y~x, span=0.7)
matplot(x,cbind(y,loe1$fit,loe2$fit,loe3$fit,loe4$fit),type="pllll",ylab="y",pch=18,lty=c(1,2,3,4),main="LOESS fit")
install.packages(c("DT", "optimx", "xfun"))
？formula
?formula
# Dichotomous Predictor and Analysis of Covariance
cathedral <- read.table("cathedral.data")
lapply(split(cathedral, cathedral$style),summary)
plot(cathedral$x,cathedral$y,type="n",xlab="Nave height",ylab="Length")
text(cathedral$x,cathedral$y,as.character(cathedral$s))
g = lm(y ~ x * style, data = cathedral) # "*"不代表乘法，(a + b + c) * (a + b + c) <=> a + b + c + a:b + a:c + b:c, ":"代表模型中的交互项
summary(g)
model.matrix(g)
?options
?contrasts
# Let's fit the full model again under (-1, 1) coding:
g <- lm(y ~ x+style+x:style, data=cathedral)
summary(g)
options(contrasts=c("contr.sum","contr.poly")) # Assign contrast
# Let's fit the full model again under (-1, 1) coding:
g <- lm(y ~ x+style+x:style, data=cathedral)
summary(g)
model.matrix(g) #Check the coding method
options(contrasts = c('contr.treatment', 'contr.poly'))
g1 <- lm(y ~ x+style, data=cathedral)
cathedral
bline(44.298,4.7116)
abline(44.298+80.393,4.7116,lty=2)
abline(44.298,4.7116)
abline(44.298+80.393,4.7116,lty=2)
par(mfrow=c(2,2))
plot(g)
# Polytomous predictor
twins <- read.table("twin.data",header=T)
plot(twins$B,twins$F,type="n",xlab="Biological IQ",ylab="Foster IQ")
# Polytomous predictor
twins <- read.table("twin.data",header=T)
plot(twins$B,twins$F,type="n",xlab="Biological IQ",ylab="Foster IQ")
# Polytomous predictor
twins <- read.table("twin.data",header=T)
plot(twins$B,twins$F,type="n",xlab="Biological IQ",ylab="Foster IQ")
text(twins$B,twins$F,substring(as.character(twins$S),1,1))
g <- lm(Foster ~ Biological*Social, twins)
summary(g)
# Now see if the model can be simplified to the parallel lines model:
gp<- lm(Foster ~ Biological+Social, twins)
anova(gp,g)
library(MASS)
boxcox(g, plotit=T)
library(MASS)
savings <- read.table("savings.data")
g <- lm(sav ~ p15 + p75 + inc + gro, data=savings)
boxcox(g, plotit=T)
?boxcox
x = boxcox(g, plotit=T)
x
?boxcox
boxcox(g, plotit=T, lambda=seq(0.5,1.5,by=0.1))
gala <- read.table("gala.data")
gg <- lm(Species~Area+Elevation+Nearest+Scruz+Adjacent, data=gala)
boxcox(gg, plotit=T)
boxcox(gg, lambda=seq(0.0,1.0,by=0.05), plotit=T)
g3 <- update(g, . ~ . + I(p15*log(p15))) ;  summary(g3) # Now see if p15 should be transformed.
remotes::install_github("anthonynorth/rscodeio")
remotes::install_github("anthonynorth/rscodeio")
rscodeio::install_theme()
install.packages('remotes')
remotes::install_github("anthonynorth/rscodeio")
rscodeio::install_theme()
install.packages("extrafont")
library(extrafont)
font_import()
library(extrafont)
font_import()
library(extrafont)
font_import()
library(extrafont)
font_import()
# Tesging-based procedure
data(state)
statedata <- data.frame(state.x77, row.names=state.abb, check.names=T)
g <- lm(Life.Exp ~ ., data=statedata)
summary(g)
# Which predictors should be included - can you tell from the p-values? Looking at the coefficients, can you see what operation would be helpful? Does the murder rate decrease life expectancy - that's obvious a priori but how should these results be interpreted?
# We illustrate the backward method --- at each step, we remove the predictor with the largest p-value over 0.05. [Note: In R, the function "mle.stepwise()" in "wle" library can automatically do the job. See the "help(mle.stepwise)" page in R for details.]
mle.stepwise(g)
# Which predictors should be included - can you tell from the p-values? Looking at the coefficients, can you see what operation would be helpful? Does the murder rate decrease life expectancy - that's obvious a priori but how should these results be interpreted?
# We illustrate the backward method --- at each step, we remove the predictor with the largest p-value over 0.05. [Note: In R, the function "mle.stepwise()" in "wle" library can automatically do the job. See the "help(mle.stepwise)" page in R for details.]
library(wle)
install.packages('wile')
install.packages('wle')
install.packages("wle")
# Criterion-based procedure
g <- lm(Life.Exp ~ ., data=statedata)
step(g)
library(leaps)
install.packages('leaps')
# Criterion-based procedure
g <- lm(Life.Exp ~ ., data=statedata)
step(g)
library(leaps)
x <- statedata[,-4]
y <- statedata[,4]
gcp <- leaps(x,y)
install.packages("esquisse")
esquisse:::esquisser()
install.packages("MASS")
esquisse:::esquisser()
esquisse:::esquisser()
install.packages("plotly")
plotly::plot_ly(x = rnorm(100, mean = 0, sd = 1), y = rnorm(100, mean = 0, sd = 1), z = rnorm(100, mean = 0, sd = 1) )
plotly::plot_ly(x = rnorm(100, mean = 0, sd = 1), y = rnorm(100, mean = 0, sd = 1), z = rnorm(100, mean = 0, sd = 1) ) %>% add_surface()
add_surface(plotly::plot_ly(x = rnorm(100, mean = 0, sd = 1), y = rnorm(100, mean = 0, sd = 1), z = rnorm(100, mean = 0, sd = 1) ))
plotly::add_surface(plotly::plot_ly(x = rnorm(100, mean = 0, sd = 1), y = rnorm(100, mean = 0, sd = 1), z = rnorm(100, mean = 0, sd = 1) ))
plotly::add_surface(plotly::plot_ly(x = rnorm(100, mean = 0, sd = 1), y = rnorm(100, mean = 0, sd = 1)))
install.packages("openssl")
install.packages("tinytex")
install.packages("brm")
install.packages("brms")
